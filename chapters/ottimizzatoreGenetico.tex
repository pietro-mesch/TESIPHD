\chapter{A Real-Time Forecast-Based Optimiser} \label{c:optimiser}

The foremost aim of this work is to exploit the versatility and speed of advanced macroscopic traffic simulation to bring forth a heuristic approach capable of improving signal plans \emph{in real time} using live reliable data and dynamic traffic forecast.\\
It represents an attempt to bring together the best of most signal setting approaches introduced in the previous chapters, in that by integration within the real-time traffic management environment outlined in section \ref{s:optima} it aims to be:
\begin{description}
\item[adaptive] \--- since real-time operation should guarantee a degree of adaptivity so far only expected of actuated signals, besting other plan-generating systems particularly in terms of response times;

\item[accurate] \--- thanks to the detailed network and traffic propagation models provided by the traffic management environment, coupled with solid real-time data, which enable it to operate on more reliable assumptions about traffic and its movements;

\item[impartial] \--- by relying on an objective-driven heuristic search method to avoid the simplifications involved in a strictly analytical approach, behaving like a feedback controller and accounting for the short-term consequences of its decisions, rather than making assumptions about the best way to operate signals optimally;

\item[versatile] \--- because once the principles of operation are proven sound and the system integration is functional, the same can be used to approach more complex optimisation problems, operate on longer time scales or be used as powerful offline planning tools;

\item[scalable] \--- by relying on task distribution and parallel computing.
\end{description}

The proposed \emph{active signal control} approach uses a Genetic Algorithm coupled with a superior macro-simulation traffic forecast engine to generate and select candidate signal timings, gradually guiding their evolution towards a global optimum that yields the best network performance on the affected area.

Previous chapters introduced the different components that allow the real-time optimisation, from the Dynamic Traffic Assignment engine to the Genetic Algorithm itself, detailing their inner workings and the importance of their contribution. The most relevant known signal optimisation approaches, a study of which drove the design of this unprecedented alternative, were also presented.

This chapter illustrates the approach in detail, describing how the different components come together and communicate to provide signal timings for optimal arterial traffic control.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Heuristic Offset Optimisation}
The task at hand is to develop an optimiser that can choose the timing \emph{offsets} (see section \ref{s:signaloffset}) between a group of adjacent signalised junctions that regulate traffic progression along a \emph{corridor} as defined in section \ref{s:corridor}.

The problem of arterial \emph{coordination} has been tackled in a variety of ways, of which some are presented in section \ref{s:signalcoordination}: mostly, as the relevant scientific literature testifies, analytical approaches to the problem have been sought which revolve around the concept of \emph{bandwidth} as the driving metric. These stem  from the extremely reasonable assumption that to increase the chance of encountering a \emph{green wave} along a traffic artery should ultimately mean to maximise its throughput; they are also a testimony to the extreme difficulty of encapsulating the complex dynamics of traffic itself into a closed-form analytical formulation that would be nearly as elegant as those that can be built upon the relatively simple paradigm of bandwidth, discussed in detail in section \ref{s:bandmax}.

Although moving \emph{from} the idea of bandwidth maximisation, as illustrated in this chapter, this work aims to do \emph{away} not only with the search for green waves but with the very need to explicitly model the correlation between signal offsets and arterial traffic fluidity. \\
The idea is to rely on simulation to verify \emph{a posteriori} whether certain timing choices bring about an improvement in performance of the corridor, and to what extent: this allows to concentrate on the \emph{results}, which can be assessed according to any chosen metric and may well arise from less obvious decisions than those which would only seek to maximise the throughput.

The one presented in this work is therefore in essence a \emph{heuristic} approach to arterial coordination, which should be able to operate in a wider range of traffic conditions thanks to the Dynamic Traffic Assignment algorithm described in Chapter \ref{c:tools} to improve articulated and sustainable performance objectives. While the objectives driving the optimisation deserve an in-depth discussion, presented in Chapter \ref{c:objectives}, the next few sections are concerned with the practical aspects of the implementation of the proposed method.


\fig{htbp}{PIX/undoguy.png}{f:run}{\todo{ciclo decision/plant/forecast/metrics}}{width=0.4\textwidth}

\subsection{Rolling Look-Ahead Window Optimisation}
Two of the strongest features of the heuristic optimisation presented are the capability to account for transient traffic phenomena (such as the gradual build-up or dissipation of queues over a number of signal cycles) and of future events which may be known in advance (such as road closures and deviations). In order to fully exploit these advantages, the optimisation is performed by evaluating traffic conditions as they develop over a \emph{rolling look-ahead window}, i.e. a time span in the order of a few signal cycles that is completely in the future with respect to the real time during which the optimisation occurs.

The boundary conditions at the beginning of the time window are known, and the optimiser evaluates the corridor performance arising from the predicted demand and tentative signal timings until a satisfactory choice is found, as illustrated in Figure \ref{f:rollingwindow} and discussed in more detail throughout this chapter.
\fig{htbp}{PIX/rollingwindow.png}{f:rollingwindow}{
Rolling Optimisation is performed in real-time on a time window in the near future, while the previous (now current) time window plays out and results of the corresponding optimisation are implemented on the street-level equipment. }{width=\textwidth}

On one hand, the look-ahead window allows to account for the short term effects of signal timing choices, protecting the optimisation from \emph{greedy} solutions that may promote a fast progression only to cause graver congestion down the line; but also to consider events that may radically change the outcome of a given set of timings.\\
These beneficial effects would call for the look-ahead window to be as long as possible, for the optimisation to be best \emph{informed}.

On the other hand, however, since signal timings are constant over the optimisation window, increasing its length reduces the responsiveness and adaptivity of the system: the point is to find the best timings for very specific traffic conditions as they arise, and it would be counter-productive to allow them to change significantly over the evaluation period, thus confusing results.

Another limiting factor is imposed by the computational requirements of the simulation, which grow more or less linearly with the number of intervals that need to be computed, as should be evident from the outline of the algorithm given in section \ref{s:TRE}: for constant time resolution, a longer look-ahead window takes longer to simulate, and it needs to be simulated in the order of a few hundred times for the genetic algorithm to yield significant improvements in performance. 

Time resolution cannot be sacrificed, as it is crucial to the correct reproduction of the rapid within-cycle queue dynamics, which are one of the main components of corridor performance indicators and would get averaged out by longer simulation intervals. For the intended real-time operation, the one on execution times is a particularly stringent limitation.

Sizing of the look-ahead window should strike a good compromise between execution times, responsiveness and control feedback, and values in the order of 10 minutes, or 5-10 signal cycles should be optimal for most applications, if the computing resources available allow it.
Rough sizing calculations are presented in the relevant section (\ref{res:windowsizing}) of the \nameref{c:results} chapter, alongside performance results for the validation tests performed.

\section{TRE as Performance Function} \label{s:rollingtre}
The Dynamic Traffic Assignment algorithm known as TRE (described in more detail in section \ref{s:TRE}) is used in this integration to provide the optimiser with the solution evaluation capabilities it requires for its own stochastic search algorithm.

It should be clear by now that the task of optimising offsets \emph{per se} does not require knowledge of the entire network, of demand profiles or of the events that may modify one or the other: the optimisation loop strictly entails solution generation and evaluation.

TRE can then serve as a \emph{single point of contact} between the traffic management system and the offset optimiser, to which it delivers all the precious information available in the only form that is really useful, i.e. that of accurate predictions about the outcome of a choice of signal offsets. This is illustrated in Figure \ref{f:trega}.

\fig{htbp}{PIX/interface.jpg}{f:trega}{ Interaction between the optimiser and TRE}{width=0.4\textwidth}

To be more precise, it should be specified that in order to perform the rolling real-time optimisation illustrated in Figure \ref{f:rollingwindow} TRE must carry out a few operations beyond the simple traffic propagation that directly yields the fitness values for the offset solutions.
It may be useful to exemplify the optimiser operation in relation to a single corridor and examine the different phases of the rolling optimisation cycle, which can be summarised as follows.
\begin{description}
\item[Step 0 - start:] when the optimiser is first launched, the only parameter it requires beyond its own algorithm configuration is the index of the corridor to be optimised. TRE starts with its own configuration, loads the model and returns to the optimiser what little corridor data it requires: number of junctions and cycle length at the very least, plus the slack-band starting offsets (refer to section \ref{s:slackband}) for GA population priming if desired (which can be calculated in seconds as a property of the corridor).
\item[Step 1 - DTA:] TRE performs a full dynamic user equilibrium assignment from the current time $t_0$, covering a span of \emph{two} optimisation windows $[t_0,t_1]\cup[t_1, t_2]$ into the future, and saves:
\begin{itemize}
\item the flows onto and out of the cordon (access and egress) links of the corridor,
\item the turn rates (averaged over the entire period) that determine the splitting of flows at each intersection all along the corridor
\item the vehicles present on each arc at the beginning of the next window, i.e. $t_1$
\end{itemize}
obviously starting itself with a \emph{loaded} network if arc occupancy data is available for the initial instant of the DTA span.
\item[Step 2 - DNL:] using the split rates just calculated, TRE propagates flows very rapidly over the \emph{optimisation window} $[t_1,t_2]$ for each set of offsets proposed by the optimiser, returning the desired performance indices: this is not an equilibrium assignment, and is performed \emph{only} on the corridor and cordon links.
\item[Step 3 - finalisation:] when the optimisation objectives are satisfied or the current time window is almost elapsed, optimised offsets are sent to the data model to be included as \emph{future events} by subsequent simulations, and handled as appropriate by Optima for their implementation on the field.
\item[Step 4 - rolling forward:] with the new offsets finalised and the state of the network known, TRE can go back to Step 1 and perform a new DTA, so that the \emph{next} window $[t_2,t_3]$ can be optimised while the one just finalised $[t_1,t_2]$ plays out in the real world.
\end{description}

In this way, TRE can rapidly evaluate hundreds of solutions using the most up-to-date supply and demand information. Details on the most crucial steps 2 and 3 are given in the next sections.

\subsection{Network Wide DTA}
The Dynamic Traffic Assignment provides the basis in terms of origin flows and split rates for the evaluation of candidate solutions, and must be performed on the whole network to fully exploit the advantages of real time traffic forecast. It is the single most time-consuming task required for the optimisation of each time window, as it involves the iterative algorithm described in section \ref{s:TRE} that searches for the User Equilibrium condition.

The latter is considered satisfied when the user route choices (depending on the arc costs, including their travel times) are in equilibrium with the arc costs (determined by the collective route choices of the users). 

The simulation engine can decouple the route choice model intervals from the flow propagation model to greatly reduce execution times, so while the latter must be numerous and short due to resolution requirements and inherent limitations of the \emph{General Link Transmission Model} (see section \ref{s:gltm}), the route choices can be averaged over the whole time window. This is a more than reasonable assumption, and in fact reflects rather well the fact that over a relatively short time window (in the order of 10 minutes) the ratio of vehicles getting on and off the corridor at each junction with respect to the total flows can be expected not to change significantly.

The results of the DTA Equilibrium step of the optimisation are therefore:
\begin{itemize}
\item \textbf{initial conditions} in the form of vehicles already on corridor and cordon arcs at the initial instant of the optimisation window;
\item \textbf{flow profiles} that enter the corridor's cordon arcs, with a fine ($\thicksim 1$ s) time resolution over the entire window;
\item \textbf{split rates} for all diversion nodes in the corridor, constant over the simulation window.
\end{itemize} 
which are the input for the Dynamic Network Loading.


\subsection{Solution Evaluation with DNL}
The Dynamic Network Loading algorithm handles the propagation in space and time of traffic flows, across the network and the optimisation window respectively. As detailed in section \ref{s:gltmflowsim}, it is also responsible of implementing the effects of time-dependent phenomena such as the capacity reductions administered by \emph{traffic signals}, producing a detailed forecast of the evolution of traffic which includes congestion, queue formation and spillback.

In order to evaluate the effects of a choice of signal offsets, DNL can be performed for each candidate solution proposed by the genetic algorithm, covering all relevant arcs (a subset of the network that only includes the optimisation corridor and its cordon arcs) for the entire span of the optimisation window with a fine time resolution, in order to capture transient traffic phenomena.

The result are cumulative profiles of the vehicles entering and leaving each arc, which are easily processed to obtain queue lengths and travel times and hence Key Performance Indicators for the corridor operating under the given signal timings. These are returned as the \emph{fitness value} used by the genetic algorithm to rank tentative offset solutions.

\section{Performance and Scalability}
Computational efficiency is of the utmost importance in a real-time environment.
As already mentioned, the aim of this optimiser is to operate in \emph{rolling horizon}, meaning that each optimisation must be carried out in a limited time, while the results of the previous are implemented. This means that, beyond all considerations about optimisation window sizing, the likely limiting factor will be the lower bound imposed by the time it takes for the optimiser to reach performance improvements that justify the effort: this will be already in the order of a few signal cycles; if resources are in excess and there is no reason to allow the genetic algorithm a longer time to optimise, the window length may then be increased slightly to enhance the look-ahead capability.

Fortunately, as detailed in the previous sections, the single most time consuming task is the initial network-wide equilibrium assignment (DTA phase) which must be performed only once; furthermore, the following circumstances alleviate its computational cost:
\begin{itemize}
\item the route choice part of the algorithm is extremely time consuming and scales badly with the problem size, but it is performed over very large time intervals, and therefore once or twice at most per optimisation;
\item both the route choice and the dynamic network loading can be \emph{significantly} sped up by parallelisation of the route choice and network loading algorithms: as attested in our \citep{attanasi2015real} and further discussed in section \ref{s:parallel}.
\end{itemize} 

Solution evaluation then relies on several short DNL performed on a comparatively \emph{minuscule} network, and a single TRE instance is already capable of effectively optimising a 10' window in rolling horizon on an ordinary computer.

The next few sections will go into more detail about the communication between the different components concurring to the corridor offset optimisation, showing how physical separation of tasks between different machines is not only possible, but can lead to significant performance improvements that should open up several venues for broadening the scope of this initial proof-of-concept application.

\subsection{Calling Method and Data Exchange}
The Optima traffic management environment (section \ref{s:optima}) is, in general, a \emph{distributed} system: the various software components illustrated for example in Figure \ref{f:optima} can be and often \emph{are} in practice located on different machines, communicating with each other over internet protocols and exchanging data via the Traffic Data Exchange central database.

This is an advantage, as it allows to distribute tasks across different inexpensive machines, but the cost of communications can quickly become prohibitive if the payloads are too large or if they need to be sent too often: even over the fastest networks available, data travels several orders of magnitude slower than it can be shared between processes on the same machine, and the connection overhead on TCP connections means that any time-critical repeated action should \emph{not} entail creating a new connection.

Fortunately, with reference to the rolling optimisation described in section \ref{s:rollingtre} of this chapter and a peek-ahead to Figure \ref{f:parallel} it is plain that the amount of data exchanged between the different components concurring to the optimisation is more than manageable, and are summarised in Table \ref{t:data} for each optimisation phase (message headers not considered).

\tabella{p{2cm} p{8cm} p{3cm}}{t:data}{DATA TRAFFIC DURING OPTIMISATION}{
 & Data Exchange & Payload Size \\ 
\hline
Startup: & TRE loads the network from the TDE database: this may be lengthy but it is only done \emph{once} on startup and doesn't weigh on optimisation & N/A \\
Step 0: & the optimiser sends a single integer corridor index plus the time window boundaries to start TRE, receiving the cycle length, the number of junctions $n = |\corridor|$ plus $n$ geometric offsets for population priming & $ < 1 kB$ \\
\hline
Step 1: & TRE reads real time data for the DTA then writes relevant results, once per window & $\sim 1-3$ Mb \\
\hline
Step 2: &  at each Genetic Algorithm generation, if the latter and TRE are on different machines, they exchange a batch of candidate solutions for as many performance indices &
$\textit{population size} \times (n+1) \times 2 \text{ bytes} \simeq 10 \text{ kB}$\\
\hline
Step 3: & the final offsets are sent to TDE & $n \times 2 \text{ bytes}$ \\
\hline
Step 4: & the optimiser sends the new time window start time and if necessary receives a new set of slackband offsets & $ 8+ n \times 2$ bytes\\
}

The above rough estimates suggest that the optimiser can easily be de-localised with respect to the machine running TRE, which in turn potentially allows to increase the number of simulator instances running in parallel to process the optimisation tasks faster, as discussed in the next section.

\subsection{Task Parallelisation} \label{s:parallel}
Although the DTA phase of the optimisation process cannot be broken down or distributed across different TRE instances to speed it up, the TRE algorithm can fully exploit the computing power of a single machine thanks to multi-threading: in our article \citep{attanasi2015real} it is shown how \emph{near-linear} performance gains can be obtained by increasing the number of threads (i.e. DTA execution times are almost inversely proportional to the number of cores). Greater returns are obtained for larger, more complex networks, which benefit the most from running on more and more threads; results from the original article for up to 16 threads on large real-world networks are presented in Appendix \ref{a:parallel}.

The performance benefits are obtained by distributing the fundamental tasks that make up the DTA algorithm across all available processor cores: 
\begin{itemize}
\item during the \textbf{route choice phase}, parallel threads handle the \emph{serial} A* single-source Dynamic Shortest Path searches that must be performed for each network zone for O-D demand routing (which is \emph{much} faster than a \emph{parallel} A* search);
\item during the \textbf{network loading phase}, threads perform node model calculations in parallel then proceed to propagate flow states on the same node's backward and forward star links.
\end{itemize}

Under these premises, and bearing in mind the necessary data exchanges summarised in Table \ref{t:data}, a few task parallelisation options appear feasible for future up-scaling of the application, as seen in Figure \ref{f:parallel}. It should be noted that from the \emph{optimiser} point of view the only relevant parameter is the problem \emph{size} i.e. the number of variables and the search space they span, while the \emph{type} of problem and the size of the area of interest affect the load on the simulator.

\fig{htbp}{PIX/parallel.png}{f:parallel}{Parallelisation Options: several single-thread TRE instances running on the same machine can speed up solution evaluation by distributing the load without competing for processor cores; by increasing the number of machines TRE can \emph{also} deal with more demanding DNL in a short time by fully exploiting multi-threading.}{width=\textwidth}

Depending on the problem type, and in all cases exploiting the extant possibility of running different TRE instances on different machines, the options to reduce the time required for optimisation are summarised as follows.
\begin{description}
\item[Distributed Serial DNL:] this is the simplest case, involving the least data transfers and additional machines; best suited to all cases where the DNL is simple enough ($ < 1000$ links) that it would not benefit from parallelisation. The possibility to evaluate \emph{batches} of candidate solutions in parallel using \emph{serial} DNL on different single-thread TRE instances may be used to improve the performance of an analogous optimisation to the one presented here, or to tackle a slightly more complicated application such as could be a corridor optimiser that can also determine phase green shares.
\item[Parallel TRE Cluster:]  if the DNL complexity is such that it \emph{would} significantly benefit from multi-threading \--- e.g. for performance evaluation over large network portions \--- a cluster of independent machines running multi-thread TRE would add to the benefits of the simpler solution (evaluation batching) the possibility to exploit all processor cores, to evaluate \emph{each} solution in a fraction of the time.
\end{description}
Notice that for reasonably sized problems and sub-networks, ordinary inexpensive four-core processors would already be more than enough to reap all the available benefits of parallel evaluations or multi-thread DNL. In any case, the DTA should be performed on a single machine (the best performing, if there is one) to obtain corridor flows and initial conditions for all consumer TRE instances tasked with solution evaluation. 

These parallel computing solutions could be easily implemented with a relatively small development effort, but could further increase the performance of this solution far beyond that of the already viable case presented in this work.
